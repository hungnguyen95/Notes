{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Variational Autoencoders (VAEs)\n",
    "\n",
    "## The neural net perspective"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In neural net language, a variational autoencoder consists of an encoder, a decoder, and a loss function.\n",
    "\n",
    "![](./images/1.png)\n",
    "\n",
    "The *encoder* is a neural network. Its input is a datapoint $x$, its output is a hidden representation $z$, and it has weights and biases $\\theta$. To be concrete, let’s say $x$ is a 28 by 28-pixel photo of a handwritten number. The encoder ‘encodes’ the data which is $784$-dimensional into a latent (hidden) representation space $z$, which is much less than $784$ dimensions. This is typically referred to as a ‘bottleneck’ because the encoder must learn an efficient compression of the data into this lower-dimensional space. Let’s denote the encoder $q_\\theta (z \\mid x)$. We note that the lower-dimensional space is stochastic: the encoder outputs parameters to $q_\\theta (z \\mid x)$, which is a Gaussian probability density. We can sample from this distribution to get noisy values of the representations zz.\n",
    "\n",
    "The decoder is another neural net. Its input is the representation $z$, it outputs the parameters to the probability distribution of the data, and has weights and biases $\\phi$. The decoder is denoted by $p_\\phi(x\\mid z)$. Running with the handwritten digit example, let’s say the photos are black and white and represent each pixel as $0$ or $1$. The probability distribution of a single pixel can be then represented using a Bernoulli distribution. The decoder gets as input the latent representation of a digit $z$ and outputs $784$ Bernoulli parameters, one for each of the $784$ pixels in the image. The decoder ‘decodes’ the real-valued numbers in $z$ into $784$ real-valued numbers between $0$ and $1$. Information from the original $784$-dimensional vector cannot be perfectly transmitted, because the decoder only has access to a summary of the information (in the form of a less-than-$784$-dimensional vector $z$). How much information is lost? We measure this using the reconstruction log-likelihood $\\log p_\\phi (x\\mid z)$ whose units are nats. This measure tells us how effectively the decoder has learned to reconstruct an input image $x$ given its latent representation $z$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The *loss function* of the variational autoencoder is the negative log-likelihood with a regularizer. Because there are no global representations that are shared by all datapoints, we can decompose the loss function into only terms that depend on a single datapoint $l_i$. The total loss is then $\\sum_{i=1}^N l_i$ for $N$ total datapoints. The loss function $l_i$ for datapoint $x_i$:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$l_i(\\theta, \\phi) = - \\mathbb{E}_{z\\sim q_\\theta(z\\mid x_i)}[\\log p_\\phi(x_i\\mid z)] + \\mathbb{KL}(q_\\theta(z\\mid x_i) \\mid\\mid p(z))$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first term is the reconstruction loss, or expected negative log-likelihood of the $i$-th datapoint. The expectation is taken with respect to the encoder’s distribution over the representations. This term encourages the decoder to learn to reconstruct the data. If the decoder’s output does not reconstruct the data well, statistically we say that the decoder parameterizes a likelihood distribution that does not place much probability mass on the true data. For example, if our goal is to model black and white images and our model places high probability on there being black spots where there are actually white spots, this will yield the worst possible reconstruction. Poor reconstruction will incur a large cost in this loss function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The second term is a regularizer that we throw in (we’ll see how it’s derived later). This is the Kullback-Leibler divergence between the encoder’s distribution $q_\\theta(z\\mid x)$. This divergence measures how much information is lost (in units of nats) when using $q$ to represent $p$. It is one measure of how close $q$ is to $p$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the variational autoencoder, $p$ is specified as a standard Normal distribution with mean zero and variance one, or $p(z) = Normal(0,1)$. If the encoder outputs representations $z$ that are different than those from a standard normal distribution, it will receive a penalty in the loss. This regularizer term means ‘keep the representations $z$ of each digit sufficiently diverse’. If we didn’t include the regularizer, the encoder could learn to cheat and give each datapoint a representation in a different region of Euclidean space. This is bad, because then two images of the same number (say a 2 written by different people, $2_{alice}$ and $2_{bob}$ could end up with very different representations $z_{alice}, z_{bob}$. We want the representation space of $z$ to be meaningful, so we penalize this behavior. This has the effect of keeping similar numbers’ representations close together (e.g. so the representations of the digit two ${z_{alice}, z_{bob}, z_{ali}}$ remain sufficiently close)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We train the variational autoencoder using gradient descent to optimize the loss with respect to the parameters of the encoder and decoder $\\theta$ and $\\phi$. For stochastic gradient descent with step size $\\rho$, the encoder parameters are updated using $\\theta \\leftarrow \\theta - \\rho \\frac{\\partial l}{\\partial \\theta}$ and the decoder is updated similarly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Variational Autoencoder Explained"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instead of mapping the input into a fixed vector, we want to map it into a distribution. Let’s label this distribution as $p_\\theta$, parameterized by $\\theta$. The relationship between the data input $x$ and the latent encoding vector $z$ can be fully defined by:\n",
    "\n",
    "- Prior $p_\\theta(\\mathbf{z})$\n",
    "- Likelihood $p_\\theta(\\mathbf{x}|\\mathbf{z})$\n",
    "- Posterior $p_\\theta(\\mathbf{z}|\\mathbf{x})$\n",
    "\n",
    "Assuming that we know the real parameter $\\theta^*$ for this distribution. In order to generate a sample that looks like a real data point $\\mathbf{x}^{(i)}$, we follow these steps:\n",
    "\n",
    "1. First, sample a $\\mathbf{z}^{(i)}$ from a prior distribution $p_{\\theta^∗}(\\mathbf{z})$.\n",
    "2. Then a value $\\mathbf{z}^{(i)}$ is generated from a conditional distribution $p_{\\theta^∗}(\\mathbf{x}|\\mathbf{z}=\\mathbf{z}^{(i)})$.\n",
    "\n",
    "The optimal parameter $\\theta^∗$ is the one that maximizes the probability of generating real data samples:\n",
    "\n",
    "$$\\theta^{*} = \\arg\\max_\\theta \\prod_{i=1}^n p_\\theta(\\mathbf{x}^{(i)})$$\n",
    "\n",
    "Commonly we use the log probabilities to convert the product on RHS to a sum:\n",
    "\n",
    "$$\\theta^{*} = \\arg\\max_\\theta \\sum_{i=1}^n \\log p_\\theta(\\mathbf{x}^{(i)})$$\n",
    "\n",
    "Unfortunately it is not easy to compute $p_\\theta(x^{(i)})$ in this way, as it is very expensive to check all the possible values of $z$ and sum them up. To narrow down the value space to facilitate faster search, we would like to introduce a new approximation function to output what is a likely code given an input $\\mathbf{x}$, $q_\\phi(\\mathbf{z}\\vert\\mathbf{x})$, parameterized by $\\phi$.\n",
    "\n",
    "<p align='center'>\n",
    "    <img src='./images/2.png'>\n",
    "</p>\n",
    "\n",
    "Now the structure looks a lot like an autoencoder:\n",
    "\n",
    "- The conditional probability $p_\\theta(x|z)$ defines a generative model, similar to the decoder. $p_\\theta(x|z)$ is also known as probabilistic decoder.\n",
    "- The approximation function $q_\\phi(z|x)$ is the probabilistic encoder, playing a similar role as encoder. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss Function: ELBO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The estimated posterior $q_\\phi(\\mathbf{z}\\vert\\mathbf{x})$ should be very close to the real one $p_\\theta(\\mathbf{z}\\vert\\mathbf{x})$. We can use [Kullback-Leibler divergence](https://en.wikipedia.org/wiki/Kullback%E2%80%93Leibler_divergence) to quantify the distance between these two distributions. KL divergence $D_\\text{KL}(X\\|Y)$ measures how much information is lost if the distribution $\\mathbf{Y}$ is used to represent $\\mathbf{X}$.\n",
    "\n",
    "In our case we want to minimize $D_\\text{KL}( q_\\phi(\\mathbf{z}\\vert\\mathbf{x}) \\| p_\\theta(\\mathbf{z}\\vert\\mathbf{x}) )$ with respect to $\\phi$.\n",
    "\n",
    "But why use $D_\\text{KL}(q_\\phi \\| p_\\theta)$ (reversed KL) instead of $D_\\text{KL}(p_\\theta \\| q_\\phi)$ (forward KL)? Eric Jang has a great explanation in his post on Bayesian Variational methods. As a quick recap:\n",
    "\n",
    "![](./images/3.png)\n",
    "\n",
    "- Forward KL divergence: $D_\\text{KL}(P\\|Q) = \\mathbb{E}_{z\\sim P(z)} \\log\\frac{P(z)}{Q(z)}$; we have to ensure that $Q(z)>0$ wherever $P(z)>0$. The optimized variational distribution $q(z)$ has to cover over the entire $p(z)$.\n",
    "- Reversed KL divergence: $D_\\text{KL}(Q\\|P) = \\mathbb{E}_{z\\sim Q(z)} \\log\\frac{Q(z)}{P(z)}$; minimizing the reversed KL divergence squeezes the $Q(z)$ under $P(z)$.\n",
    "\n",
    "Let’s now expand the equation:\n",
    "\n",
    "\\begin{aligned}\n",
    "& D_\\text{KL}( q_\\phi(\\mathbf{z}\\vert\\mathbf{x}) \\| p_\\theta(\\mathbf{z}\\vert\\mathbf{x}) ) & \\\\\n",
    "&=\\int q_\\phi(\\mathbf{z} \\vert \\mathbf{x}) \\log\\frac{q_\\phi(\\mathbf{z} \\vert \\mathbf{x})}{p_\\theta(\\mathbf{z} \\vert \\mathbf{x})} d\\mathbf{z} & \\\\\n",
    "&=\\int q_\\phi(\\mathbf{z} \\vert \\mathbf{x}) \\log\\frac{q_\\phi(\\mathbf{z} \\vert \\mathbf{x})p_\\theta(\\mathbf{x})}{p_\\theta(\\mathbf{z}, \\mathbf{x})} d\\mathbf{z} & \\scriptstyle{\\text{; Because }p(z \\vert x) = p(z, x) / p(x)} \\\\\n",
    "&=\\int q_\\phi(\\mathbf{z} \\vert \\mathbf{x}) \\big( \\log p_\\theta(\\mathbf{x}) + \\log\\frac{q_\\phi(\\mathbf{z} \\vert \\mathbf{x})}{p_\\theta(\\mathbf{z}, \\mathbf{x})} \\big) d\\mathbf{z} & \\\\\n",
    "&=\\log p_\\theta(\\mathbf{x}) + \\int q_\\phi(\\mathbf{z} \\vert \\mathbf{x})\\log\\frac{q_\\phi(\\mathbf{z} \\vert \\mathbf{x})}{p_\\theta(\\mathbf{z}, \\mathbf{x})} d\\mathbf{z} & \\scriptstyle{\\text{; Because }\\int q(z \\vert x) dz = 1}\\\\\n",
    "&=\\log p_\\theta(\\mathbf{x}) + \\int q_\\phi(\\mathbf{z} \\vert \\mathbf{x})\\log\\frac{q_\\phi(\\mathbf{z} \\vert \\mathbf{x})}{p_\\theta(\\mathbf{x}\\vert\\mathbf{z})p_\\theta(\\mathbf{z})} d\\mathbf{z} & \\scriptstyle{\\text{; Because }p(z, x) = p(x \\vert z) p(z)} \\\\\n",
    "&=\\log p_\\theta(\\mathbf{x}) + \\mathbb{E}_{\\mathbf{z}\\sim q_\\phi(\\mathbf{z} \\vert \\mathbf{x})}[\\log \\frac{q_\\phi(\\mathbf{z} \\vert \\mathbf{x})}{p_\\theta(\\mathbf{z})} - \\log p_\\theta(\\mathbf{x} \\vert \\mathbf{z})] &\\\\\n",
    "&=\\log p_\\theta(\\mathbf{x}) + D_\\text{KL}(q_\\phi(\\mathbf{z}\\vert\\mathbf{x}) \\| p_\\theta(\\mathbf{z})) - \\mathbb{E}_{\\mathbf{z}\\sim q_\\phi(\\mathbf{z}\\vert\\mathbf{x})}\\log p_\\theta(\\mathbf{x}\\vert\\mathbf{z}) &\n",
    "\\end{aligned}\n",
    "\n",
    "So we have:\n",
    "$$D_\\text{KL}( q_\\phi(\\mathbf{z}\\vert\\mathbf{x}) \\| p_\\theta(\\mathbf{z}\\vert\\mathbf{x}) ) =\\log p_\\theta(\\mathbf{x}) + D_\\text{KL}(q_\\phi(\\mathbf{z}\\vert\\mathbf{x}) \\| p_\\theta(\\mathbf{z})) - \\mathbb{E}_{\\mathbf{z}\\sim q_\\phi(\\mathbf{z}\\vert\\mathbf{x})}\\log p_\\theta(\\mathbf{x}\\vert\\mathbf{z})$$\n",
    "\n",
    "Once rearrange the left and right hand side of the equation,\n",
    "$$\\log p_\\theta(\\mathbf{x}) - D_\\text{KL}( q_\\phi(\\mathbf{z}\\vert\\mathbf{x}) \\| p_\\theta(\\mathbf{z}\\vert\\mathbf{x}) ) = \\mathbb{E}_{\\mathbf{z}\\sim q_\\phi(\\mathbf{z}\\vert\\mathbf{x})}\\log p_\\theta(\\mathbf{x}\\vert\\mathbf{z}) - D_\\text{KL}(q_\\phi(\\mathbf{z}\\vert\\mathbf{x}) \\| p_\\theta(\\mathbf{z}))$$\n",
    "\n",
    "The LHS of the equation is exactly what we want to maximize when learning the true distributions: we want to maximize the (log-)likelihood of generating real data (that is $\\log p_\\theta(\\mathbf{x})$) and also minimize the difference between the real and estimated posterior distributions (the term $D_\\text{KL}$ works like a regularizer). Note that $p_\\theta(\\mathbf{x})$ is fixed with respect to $q_\\phi$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The negation of the above defines our loss function:\n",
    "$\\begin{aligned}\n",
    "L_\\text{VAE}(\\theta, \\phi) \n",
    "&= -\\log p_\\theta(\\mathbf{x}) + D_\\text{KL}( q_\\phi(\\mathbf{z}\\vert\\mathbf{x}) \\| p_\\theta(\\mathbf{z}\\vert\\mathbf{x}) )\\\\\n",
    "&= - \\mathbb{E}_{\\mathbf{z} \\sim q_\\phi(\\mathbf{z}\\vert\\mathbf{x})} \\log p_\\theta(\\mathbf{x}\\vert\\mathbf{z}) + D_\\text{KL}( q_\\phi(\\mathbf{z}\\vert\\mathbf{x}) \\| p_\\theta(\\mathbf{z}) ) \\\\\n",
    "\\theta^{*}, \\phi^{*} &= \\arg\\min_{\\theta, \\phi} L_\\text{VAE}\n",
    "\\end{aligned}$\n",
    "\n",
    "In Variational Bayesian methods, this loss function is known as the variational lower bound, or evidence lower bound. The “lower bound” part in the name comes from the fact that KL divergence is always non-negative and thus $-L_\\text{VAE}$ is the lower bound of $\\log p_\\theta (\\mathbf{x})$.\n",
    "\n",
    "$$-L_\\text{VAE} = \\log p_\\theta(\\mathbf{x}) - D_\\text{KL}( q_\\phi(\\mathbf{z}\\vert\\mathbf{x}) \\| p_\\theta(\\mathbf{z}\\vert\\mathbf{x}) ) \\leq \\log p_\\theta(\\mathbf{x})$$\n",
    "\n",
    "Therefore by minimizing the loss, we are maximizing the lower bound of the probability of generating real data samples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reparameterization Trick\n",
    "\n",
    "The expectation term in the loss function invokes generating samples from $\\mathbf{z} \\sim q_\\phi(\\mathbf{z}\\vert\\mathbf{x})$. Sampling is a stochastic process and therefore we cannot backpropagate the gradient. To make it trainable, the reparameterization trick is introduced: It is often possible to express the random variable z as a deterministic variable $\\mathbf{z} = \\mathcal{T}_\\phi(\\mathbf{x}, \\boldsymbol{\\epsilon})$, where $\\boldsymbol{\\epsilon}$ is an auxiliary independent random variable, and the transformation function $\\mathcal{T}_\\phi$ parameterized by $\\phi$ converts $\\epsilon$ to $\\mathbf{z}$.\n",
    "\n",
    "For example, a common choice of the form of $q_\\phi(\\mathbf{z}\\vert\\mathbf{x})$ is a multivariate Gaussian with a diagonal covariance structure:\n",
    "$\\begin{aligned}\n",
    "\\mathbf{z} &\\sim q_\\phi(\\mathbf{z}\\vert\\mathbf{x}^{(i)}) = \\mathcal{N}(\\mathbf{z}; \\boldsymbol{\\mu}^{(i)}, \\boldsymbol{\\sigma}^{2(i)}\\boldsymbol{I}) & \\\\\n",
    "\\mathbf{z} &= \\boldsymbol{\\mu} + \\boldsymbol{\\sigma} \\odot \\boldsymbol{\\epsilon} \\text{, where } \\boldsymbol{\\epsilon} \\sim \\mathcal{N}(0, \\boldsymbol{I}) & \\scriptstyle{\\text{; Reparameterization trick.}}\n",
    "\\end{aligned}$\n",
    "\n",
    "where $\\odot$ refers to element-wise product.\n",
    "\n",
    "![](./images/4.png)\n",
    "\n",
    "The reparameterization trick works for other types of distributions too, not only Gaussian. In the multivariate Gaussian case, we make the model trainable by learning the mean and variance of the distribution, $\\mu$ and $\\sigma$, explicitly using the reparameterization trick, while the stochasticity remains in the random variable $\\boldsymbol{\\epsilon} \\sim \\mathcal{N}(0, \\boldsymbol{I})$.\n",
    "\n",
    "![](./images/5.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is a code for VAE.\n",
    "You can see KL Divergence loss is: `0.5 * K.sum(K.exp(log_sigma) + K.square(mu) - 1. - log_sigma, axis=1)`\n",
    "\n",
    "Here is explain:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](./images/6.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In practice, however, it’s better to model `Σ(X)` as `logΣ(X)`, as it is more numerically stable to take exponent compared to computing log. Hence, our final KL divergence term is:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](./images/7.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation in Keras\n",
    "\n",
    "First, let’s implement the encoder net $Q(z \\mid X)$, which takes input $X$ and outputting two things: $\\mu(X)$ and $\\Sigma(X)$, the parameters of the Gaussian."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "from keras.layers import Input, Dense, Lambda\n",
    "from keras.models import Model\n",
    "from keras.objectives import binary_crossentropy\n",
    "from keras.callbacks import LearningRateScheduler\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import keras.backend as K\n",
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "m = 50\n",
    "n_z = 2\n",
    "n_epoch = 10\n",
    "\n",
    "\n",
    "# Q(z|X) -- encoder inputs = Input(shape=(784,))\n",
    "h_q = Dense(512, activation='relu')(inputs)\n",
    "mu = Dense(n_z, activation='linear')(h_q)\n",
    "log_sigma = Dense(n_z, activation='linear')(h_q)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That is, our $Q(z \\mid X)$ is a neural net with one hidden layer. In this implementation, our latent variable is two dimensional, so that we could easily visualize it. In practice though, more dimension in latent variable should be better.\n",
    "\n",
    "However, we are now facing a problem. How do we get $z$ from the encoder outputs? Obviously we could sample $z$ from a Gaussian which parameters are the outputs of the encoder. Alas, sampling directly won’t do, if we want to train VAE with gradient descent as the sampling operation doesn’t have gradient!\n",
    "\n",
    "There is, however a trick called reparameterization trick, which makes the network differentiable. Reparameterization trick basically divert the non-differentiable operation out of the network, so that, even though we still involve a thing that is non-differentiable, at least it is out of the network, hence the network could still be trained.\n",
    "\n",
    "The reparameterization trick is as follows. Recall, if we have $x \\sim N(\\mu, \\Sigma)$ and then standardize it so that $\\mu = 0, \\Sigma = 1$, we could revert it back to the original distribution by reverting the standardization process. Hence, we have this equation:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$x = \\mu + \\Sigma^{\\frac{1}{2}}x_{std}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With that in mind, we could extend it. If we sample from a standard normal distribution, we could convert it to any Gaussian we want if we know the mean and the variance. Hence we could implement our sampling operation of $z$ by:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$z = \\mu(X) + \\Sigma^{\\frac{1}{2}}(X){\\epsilon}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "where $\\epsilon \\sim N(0,1)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, during backpropagation, we don’t care anymore with the sampling process, as it is now outside of the network, i.e. doesn’t depend on anything in the net, hence the gradient won’t flow through it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_z(args):\n",
    "    mu, log_sigma = args\n",
    "    eps = K.random_normal(shape=(m, n_z), mean=0., std=1.)\n",
    "    return mu + K.exp(log_sigma / 2) * eps\n",
    "\n",
    "\n",
    "# Sample z ~ Q(z|X) z = Lambda(sample_z)([mu, log_sigma])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we create the decoder net $P(X \\mid z)$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# P(X|z) -- decoder decoder_hidden = Dense(512, activation='relu')\n",
    "decoder_out = Dense(784, activation='sigmoid')\n",
    "\n",
    "h_p = decoder_hidden(z)\n",
    "outputs = decoder_out(h_p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lastly, from this model, we can do three things: reconstruct inputs, encode inputs into latent variables, and generate data from latent variable. So, we have three Keras models:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Overall VAE model, for reconstruction and training vae = Model(inputs, outputs)\n",
    "\n",
    "# Encoder model, to encode input into latent variable # We use the mean as the output as it is the center point, the representative of the gaussian encoder = Model(inputs, mu)\n",
    "\n",
    "# Generator model, generate new data given latent variable z d_in = Input(shape=(n_z,))\n",
    "d_h = decoder_hidden(d_in)\n",
    "d_out = decoder_out(d_h)\n",
    "decoder = Model(d_in, d_out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we need to translate our loss into Keras code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vae_loss(y_true, y_pred):\n",
    "    \"\"\" Calculate loss = reconstruction loss + KL loss for each data in minibatch \"\"\"\n",
    "    # E[log P(X|z)]     recon = K.sum(K.binary_crossentropy(y_pred, y_true), axis=1)\n",
    "    # D_KL(Q(z|X) || P(z|X)); calculate in closed form as both dist. are Gaussian     kl = 0.5 * K.sum(K.exp(log_sigma) + K.square(mu) - 1. - log_sigma, axis=1)\n",
    "\n",
    "    return recon + kl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and then train it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vae.compile(optimizer='adam', loss=vae_loss)\n",
    "vae.fit(X_train, X_train, batch_size=m, nb_epoch=n_epoch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And that’s it, the implementation of VAE in Keras!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reference\n",
    "\n",
    "1. [From Autoencoder to Beta-VAE](https://lilianweng.github.io/lil-log/2018/08/12/from-autoencoder-to-beta-vae.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
